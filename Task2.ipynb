{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73305d8a",
   "metadata": {},
   "source": [
    "Assignment 2 -Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a497f",
   "metadata": {},
   "source": [
    "Import Packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd7e815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c9e33",
   "metadata": {},
   "source": [
    " Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ccff5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Edge Data:\n",
      "       source      target\n",
      "0  19585800.0  19590700.0\n",
      "1  19585800.0  19595200.0\n",
      "2  19590700.0  19592400.0\n",
      "3  19590700.0  19595200.0\n",
      "4  19591900.0  19592201.0\n",
      "\n",
      "Processed Node Data:\n",
      "    node_id  current_x  current_y  ... previous_y future_x  future_y\n",
      "0  19502500    40972.0   -16957.0  ...   -16957.0  41185.0  -16480.0\n",
      "1  19585800    12688.0    -6816.0  ...    -6816.0  13381.0   -7427.0\n",
      "2  19590700    12888.0    -6249.0  ...    -6249.0  13540.0   -6865.0\n",
      "3  19591900     8934.0    -3797.0  ...    -3797.0   9006.0   -4048.0\n",
      "4  19592201    10095.0    -5080.0  ...    -5080.0  10712.0   -5615.0\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Path to your dataset folder\n",
    "data_dir = 'dataset'\n",
    "\n",
    "\n",
    "def read_edge_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"source\", \"target\"]\n",
    "    return df\n",
    "\n",
    "# Function to read node information files\n",
    "def read_node_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"node_id\", \"current_x\", \"current_y\", \"previous_x\", \"previous_y\", \"future_x\", \"future_y\"]\n",
    "    return df\n",
    "\n",
    "edge_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".edges\")]\n",
    "all_edges = pd.DataFrame()\n",
    "for path in edge_paths:\n",
    "    all_edges = pd.concat([all_edges, read_edge_file(path)], ignore_index=True)\n",
    "\n",
    "\n",
    "node_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".nodes\")]\n",
    "all_nodes = pd.DataFrame()\n",
    "for path in node_paths:\n",
    "    all_nodes = pd.concat([all_nodes, read_node_file(path)], ignore_index=True)\n",
    "\n",
    "all_nodes.replace(to_replace='_', value=np.nan, inplace=True)\n",
    "\n",
    "required_fields = [\"future_x\", \"future_y\", \"previous_x\", \"previous_y\"]\n",
    "clean_nodes = all_nodes.dropna(subset=required_fields).reset_index(drop=True)\n",
    "\n",
    "all_edges.replace(-1, np.nan, inplace=True)\n",
    "all_edges.dropna(inplace=True)\n",
    "\n",
    "valid_node_ids = set(clean_nodes[\"node_id\"].unique())\n",
    "edge_nodes = set(all_edges[\"source\"]).union(set(all_edges[\"target\"]))\n",
    "missing_nodes = edge_nodes.difference(valid_node_ids)\n",
    "\n",
    "filtered_edges = all_edges[\n",
    "    ~all_edges[\"source\"].isin(missing_nodes) & ~all_edges[\"target\"].isin(missing_nodes)\n",
    "]\n",
    "\n",
    "reversed_edges = filtered_edges.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "edges = pd.concat([filtered_edges, reversed_edges], ignore_index=True)\n",
    "\n",
    "# Final cleaned nodes and edges\n",
    "nodes = clean_nodes\n",
    "\n",
    "# Display the result\n",
    "print(\"Processed Edge Data:\")\n",
    "print(edges.head())\n",
    "\n",
    "print(\"\\nProcessed Node Data:\")\n",
    "print(nodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb320f79",
   "metadata": {},
   "source": [
    "Normalize IDs and Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Node Feature Values:\n",
      "   current_x  current_y  previous_x  previous_y\n",
      "0   0.925234   0.172395    0.927256    0.174749\n",
      "1   0.581439   0.405838    0.586045    0.413002\n",
      "2   0.583870   0.418890    0.588457    0.426323\n",
      "3   0.535809   0.475334    0.540757    0.483930\n",
      "4   0.549921   0.445800    0.554763    0.453787\n"
     ]
    }
   ],
   "source": [
    "node_id_map = {original_id: new_index for new_index, original_id in enumerate(sorted(nodes[\"node_id\"].unique()))}\n",
    "\n",
    "nodes[\"node_id\"] = nodes[\"node_id\"].map(node_id_map)\n",
    "edges[\"source\"] = edges[\"source\"].map(node_id_map)\n",
    "edges[\"target\"] = edges[\"target\"].map(node_id_map)\n",
    "\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "\n",
    "# Copy raw targets first\n",
    "raw_targets = nodes[[\"future_x\", \"future_y\"]].copy()  \n",
    "\n",
    "# Fit target scaler first using raw targets\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(raw_targets)\n",
    "\n",
    "# Now scale only future_x, future_y using the same scaler\n",
    "nodes[[\"future_x\", \"future_y\"]] = target_scaler.transform(raw_targets)\n",
    "\n",
    "# Apply feature scaling separately\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "feature_scaler = MinMaxScaler()\n",
    "nodes[features_to_scale] = feature_scaler.fit_transform(nodes[features_to_scale])\n",
    "\n",
    "\n",
    "print(\"Scaled Node Feature Values:\")\n",
    "print(nodes[features_to_scale].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f201b",
   "metadata": {},
   "source": [
    "Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0924a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(nodes.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = nodes.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = nodes.iloc[random_indices[len(random_indices) // 2 :]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d379b",
   "metadata": {},
   "source": [
    "Prepare the Graph data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d239707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:\t\t (5840, 2)\n",
      "Node features shape: (2171, 4)\n"
     ]
    }
   ],
   "source": [
    "train_indices = train_data[\"node_id\"].to_numpy()\n",
    "test_indices = test_data[\"node_id\"].to_numpy()\n",
    "\n",
    "train_labels = train_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "test_labels = test_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "\n",
    "# Define graph representation\n",
    "edges = tf.convert_to_tensor(edges[[\"source\", \"target\"]].to_numpy(), dtype=tf.int32)\n",
    "node_states = tf.convert_to_tensor(nodes.sort_values(\"node_id\").iloc[:, 1:-2].to_numpy(), dtype=tf.float32)\n",
    "\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c4b25",
   "metadata": {},
   "source": [
    "Build the Graph Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "556c0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(node_states_expanded, (tf.shape(edges)[0], -1))\n",
    "        attention_scores = tf.nn.leaky_relu(tf.matmul(node_states_expanded, self.kernel_attention))\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\")))\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67914a",
   "metadata": {},
   "source": [
    "Multi-Head attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11369ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"average\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "        outputs = [attention_layer([atom_features, pair_indices]) for attention_layer in self.attention_layers]\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f36dc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        \n",
    "        # Additional layers as per task 2\n",
    "        self.additional_layer1 = layers.Dense(hidden_units, activation=\"relu\")\n",
    "        self.additional_layer2 = layers.Dense(hidden_units, activation=\"relu\")\n",
    "        \n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        x = self.additional_layer1(x)\n",
    "        x = self.additional_layer2(x)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.gather(outputs, indices)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc05c7f",
   "metadata": {},
   "source": [
    "Define HyperPrameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "465b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = 2\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a1a5e",
   "metadata": {},
   "source": [
    "Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a037ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Hyperparameter Tuning\n",
      "Testing different attention head counts...\n",
      "\n",
      " Training model with 4 attention heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 - 16s - 4s/step - euclidean_dist: 0.5946 - loss: 0.1565 - val_loss: 0.1603\n",
      "Epoch 2/100\n",
      "4/4 - 1s - 343ms/step - euclidean_dist: 0.5797 - loss: 0.1634 - val_loss: 0.1703\n",
      "Epoch 3/100\n",
      "4/4 - 1s - 350ms/step - euclidean_dist: 0.5600 - loss: 0.1747 - val_loss: 0.1836\n",
      "Epoch 4/100\n",
      "4/4 - 1s - 344ms/step - euclidean_dist: 0.5379 - loss: 0.1888 - val_loss: 0.1989\n",
      "Epoch 5/100\n",
      "4/4 - 1s - 344ms/step - euclidean_dist: 0.5155 - loss: 0.2043 - val_loss: 0.2150\n",
      "Epoch 6/100\n",
      "4/4 - 1s - 342ms/step - euclidean_dist: 0.4937 - loss: 0.2205 - val_loss: 0.2310\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step\n",
      " Heads = 4 Test Euclidean Distance: 44.80 m\n",
      "\n",
      " Training model with 8 attention heads...\n",
      "Epoch 1/100\n",
      "4/4 - 26s - 6s/step - euclidean_dist: 0.7330 - loss: 0.0722 - val_loss: 0.0736\n",
      "Epoch 2/100\n",
      "4/4 - 3s - 634ms/step - euclidean_dist: 0.7264 - loss: 0.0747 - val_loss: 0.0772\n",
      "Epoch 3/100\n",
      "4/4 - 3s - 639ms/step - euclidean_dist: 0.7176 - loss: 0.0789 - val_loss: 0.0822\n",
      "Epoch 4/100\n",
      "4/4 - 3s - 640ms/step - euclidean_dist: 0.7072 - loss: 0.0841 - val_loss: 0.0879\n",
      "Epoch 5/100\n",
      "4/4 - 3s - 630ms/step - euclidean_dist: 0.6955 - loss: 0.0901 - val_loss: 0.0943\n",
      "Epoch 6/100\n",
      "4/4 - 3s - 627ms/step - euclidean_dist: 0.6833 - loss: 0.0966 - val_loss: 0.1011\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 237ms/step\n",
      " Heads = 8 Test Euclidean Distance: 57.04 m\n",
      "\n",
      " Training model with 16 attention heads...\n",
      "Epoch 1/100\n",
      "4/4 - 54s - 14s/step - euclidean_dist: 0.8019 - loss: 0.0164 - val_loss: 0.0182\n",
      "Epoch 2/100\n",
      "4/4 - 5s - 1s/step - euclidean_dist: 0.7943 - loss: 0.0199 - val_loss: 0.0233\n",
      "Epoch 3/100\n",
      "4/4 - 5s - 1s/step - euclidean_dist: 0.7845 - loss: 0.0255 - val_loss: 0.0298\n",
      "Epoch 4/100\n",
      "4/4 - 5s - 1s/step - euclidean_dist: 0.7731 - loss: 0.0325 - val_loss: 0.0375\n",
      "Epoch 5/100\n",
      "4/4 - 5s - 1s/step - euclidean_dist: 0.7608 - loss: 0.0404 - val_loss: 0.0459\n",
      "Epoch 6/100\n",
      "4/4 - 5s - 1s/step - euclidean_dist: 0.7482 - loss: 0.0490 - val_loss: 0.0544\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 478ms/step\n",
      " Heads = 16 Test Euclidean Distance: 62.34 m\n"
     ]
    }
   ],
   "source": [
    "# Custom Euclidean Distance metric\n",
    "def euclidean_dist(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=-1))\n",
    "euclidean_dist.__name__ = \"euclidean_dist\"\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "#  Hyperparameters\n",
    "attention_heads = [4, 8, 16]\n",
    "results = {}\n",
    "\n",
    "print(\"Task 2: Hyperparameter Tuning\")\n",
    "print(\"Testing different attention head counts...\")\n",
    "\n",
    "# Loop over attention head settings\n",
    "for num_heads in attention_heads:\n",
    "    print(f\"\\n Training model with {num_heads} attention heads...\")\n",
    "\n",
    "    # Build model\n",
    "    gat_model = GraphAttentionNetwork(\n",
    "        node_states, edges, HIDDEN_UNITS, num_heads, NUM_LAYERS, OUTPUT_DIM\n",
    "    )\n",
    "\n",
    "    # Optimizer setup\n",
    "    optimizer = keras.optimizers.SGD(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    gat_model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=[euclidean_dist]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    gat_model.fit(\n",
    "        x=train_indices,\n",
    "        y=train_labels,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_scaled = gat_model.predict(test_indices)\n",
    "    y_true_scaled = test_labels\n",
    "\n",
    "    # Inverse transform to get original values\n",
    "    y_pred_denorm = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true_denorm = target_scaler.inverse_transform(y_true_scaled)\n",
    "\n",
    "    # Calculate Euclidean distance in original units (meters)\n",
    "    euclidean_dist_m = np.linalg.norm(y_true_denorm - y_pred_denorm, axis=1).mean()\n",
    "\n",
    "    results[num_heads] = euclidean_dist_m\n",
    "    print(f\" Heads = {num_heads} Test Euclidean Distance: {euclidean_dist_m / 1000:.2f} m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
