{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73305d8a",
   "metadata": {},
   "source": [
    "Assignment 2 -Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcaa881",
   "metadata": {},
   "source": [
    "Import Packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd7e815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c9e33",
   "metadata": {},
   "source": [
    " Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ccff5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Edge Data:\n",
      "       source      target\n",
      "0  19585800.0  19590700.0\n",
      "1  19585800.0  19595200.0\n",
      "2  19590700.0  19592400.0\n",
      "3  19590700.0  19595200.0\n",
      "4  19591900.0  19592201.0\n",
      "\n",
      "Processed Node Data:\n",
      "    node_id  current_x  current_y  ... previous_y future_x  future_y\n",
      "0  19502500    40972.0   -16957.0  ...   -16957.0  41185.0  -16480.0\n",
      "1  19585800    12688.0    -6816.0  ...    -6816.0  13381.0   -7427.0\n",
      "2  19590700    12888.0    -6249.0  ...    -6249.0  13540.0   -6865.0\n",
      "3  19591900     8934.0    -3797.0  ...    -3797.0   9006.0   -4048.0\n",
      "4  19592201    10095.0    -5080.0  ...    -5080.0  10712.0   -5615.0\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Path to your dataset folder\n",
    "data_dir = r\"D:\\DL_Assignment2\\DL_Assignment2\\dataset (1)\\dataset\"\n",
    "\n",
    "def read_edge_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"source\", \"target\"]\n",
    "    return df\n",
    "\n",
    "# Function to read node information files\n",
    "def read_node_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"node_id\", \"current_x\", \"current_y\", \"previous_x\", \"previous_y\", \"future_x\", \"future_y\"]\n",
    "    return df\n",
    "\n",
    "edge_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".edges\")]\n",
    "all_edges = pd.DataFrame()\n",
    "for path in edge_paths:\n",
    "    all_edges = pd.concat([all_edges, read_edge_file(path)], ignore_index=True)\n",
    "\n",
    "\n",
    "node_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".nodes\")]\n",
    "all_nodes = pd.DataFrame()\n",
    "for path in node_paths:\n",
    "    all_nodes = pd.concat([all_nodes, read_node_file(path)], ignore_index=True)\n",
    "\n",
    "all_nodes.replace(to_replace='_', value=np.nan, inplace=True)\n",
    "\n",
    "required_fields = [\"future_x\", \"future_y\", \"previous_x\", \"previous_y\"]\n",
    "clean_nodes = all_nodes.dropna(subset=required_fields).reset_index(drop=True)\n",
    "\n",
    "all_edges.replace(-1, np.nan, inplace=True)\n",
    "all_edges.dropna(inplace=True)\n",
    "\n",
    "valid_node_ids = set(clean_nodes[\"node_id\"].unique())\n",
    "edge_nodes = set(all_edges[\"source\"]).union(set(all_edges[\"target\"]))\n",
    "missing_nodes = edge_nodes.difference(valid_node_ids)\n",
    "\n",
    "filtered_edges = all_edges[\n",
    "    ~all_edges[\"source\"].isin(missing_nodes) & ~all_edges[\"target\"].isin(missing_nodes)\n",
    "]\n",
    "\n",
    "reversed_edges = filtered_edges.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "edges = pd.concat([filtered_edges, reversed_edges], ignore_index=True)\n",
    "\n",
    "nodes = clean_nodes\n",
    "\n",
    "print(\"Processed Edge Data:\")\n",
    "print(edges.head())\n",
    "\n",
    "print(\"\\nProcessed Node Data:\")\n",
    "print(nodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb320f79",
   "metadata": {},
   "source": [
    "Node ID Mapping and Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f76a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Node Feature Values:\n",
      "   current_x  current_y  previous_x  previous_y\n",
      "0   0.925234   0.172395    0.927256    0.174749\n",
      "1   0.581439   0.405838    0.586045    0.413002\n",
      "2   0.583870   0.418890    0.588457    0.426323\n",
      "3   0.535809   0.475334    0.540757    0.483930\n",
      "4   0.549921   0.445800    0.554763    0.453787\n"
     ]
    }
   ],
   "source": [
    "node_id_map = {original_id: new_index for new_index, original_id in enumerate(sorted(nodes[\"node_id\"].unique()))}\n",
    "\n",
    "nodes[\"node_id\"] = nodes[\"node_id\"].map(node_id_map)\n",
    "edges[\"source\"] = edges[\"source\"].map(node_id_map)\n",
    "edges[\"target\"] = edges[\"target\"].map(node_id_map)\n",
    "\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "\n",
    "# Copy raw targets first\n",
    "raw_targets = nodes[[\"future_x\", \"future_y\"]].copy()  # <-- Must be before any scaling\n",
    "\n",
    "# Fit target scaler first using raw targets\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(raw_targets)\n",
    "\n",
    "# Now scale only future_x, future_y using the same scaler\n",
    "nodes[[\"future_x\", \"future_y\"]] = target_scaler.transform(raw_targets)\n",
    "\n",
    "# Apply feature scaling separately\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "feature_scaler = MinMaxScaler()\n",
    "nodes[features_to_scale] = feature_scaler.fit_transform(nodes[features_to_scale])\n",
    "\n",
    "\n",
    "print(\"Scaled Node Feature Values:\")\n",
    "print(nodes[features_to_scale].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f201b",
   "metadata": {},
   "source": [
    "Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b0924a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(nodes.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = nodes.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = nodes.iloc[random_indices[len(random_indices) // 2 :]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d379b",
   "metadata": {},
   "source": [
    "Prepare the Graph data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d239707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:\t\t (5840, 2)\n",
      "Node features shape: (2171, 4)\n"
     ]
    }
   ],
   "source": [
    "train_indices = train_data[\"node_id\"].to_numpy()\n",
    "test_indices = test_data[\"node_id\"].to_numpy()\n",
    "\n",
    "train_labels = train_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "test_labels = test_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "edges = tf.convert_to_tensor(edges[[\"source\", \"target\"]].to_numpy(), dtype=tf.int32)\n",
    "node_states = tf.convert_to_tensor(nodes.sort_values(\"node_id\").iloc[:, 1:-2].to_numpy(), dtype=tf.float32)\n",
    "\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c4b25",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "556c0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(\n",
    "            node_states_expanded, (tf.shape(edges)[0], -1)\n",
    "        )\n",
    "        attention_scores = tf.nn.leaky_relu(\n",
    "            tf.matmul(node_states_expanded, self.kernel_attention)\n",
    "        )\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(\n",
    "            attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\"))\n",
    "        )\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67914a",
   "metadata": {},
   "source": [
    "Multi-Head attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11369ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f36dc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        return tf.gather(outputs, indices)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc05c7f",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "465b9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: Task1\n",
      "Epoch 1/100\n",
      "4/4 - 26s - 6s/step - euclidean_dist: 1.1544 - loss: 0.8250 - val_loss: 0.7712\n",
      "Epoch 2/100\n",
      "4/4 - 1s - 278ms/step - euclidean_dist: 0.9651 - loss: 0.7129 - val_loss: 0.6052\n",
      "Epoch 3/100\n",
      "4/4 - 1s - 281ms/step - euclidean_dist: 0.7471 - loss: 0.5258 - val_loss: 0.3957\n",
      "Epoch 4/100\n",
      "4/4 - 1s - 293ms/step - euclidean_dist: 0.6168 - loss: 0.3159 - val_loss: 0.1789\n",
      "Epoch 5/100\n",
      "4/4 - 1s - 247ms/step - euclidean_dist: 0.7027 - loss: 0.1345 - val_loss: 0.0874\n",
      "Epoch 6/100\n",
      "4/4 - 1s - 268ms/step - euclidean_dist: 0.6590 - loss: 0.0944 - val_loss: 0.1240\n",
      "Epoch 7/100\n",
      "4/4 - 1s - 273ms/step - euclidean_dist: 0.5772 - loss: 0.1605 - val_loss: 0.2304\n",
      "Epoch 8/100\n",
      "4/4 - 1s - 258ms/step - euclidean_dist: 0.5918 - loss: 0.2633 - val_loss: 0.3052\n",
      "Epoch 9/100\n",
      "4/4 - 1s - 190ms/step - euclidean_dist: 0.5653 - loss: 0.3054 - val_loss: 0.2935\n",
      "Epoch 10/100\n",
      "4/4 - 1s - 171ms/step - euclidean_dist: 0.5301 - loss: 0.2724 - val_loss: 0.2383\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step\n",
      "\n",
      "Test Euclidean Distance (in meters): 48.34 m\n"
     ]
    }
   ],
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = 2\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "def euclidean_dist(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=-1))\n",
    "\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM, clipnorm=1.0)\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
    ")\n",
    "\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[euclidean_dist])\n",
    "\n",
    "print('Training started: Task1')\n",
    "\n",
    "x = gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "results = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0, return_dict=True)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_scaled = gat_model.predict(test_indices)\n",
    "y_true_scaled = test_labels\n",
    "\n",
    "# Inverse transform to get original values\n",
    "y_pred_denorm = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true_denorm = target_scaler.inverse_transform(y_true_scaled)\n",
    "\n",
    "# Calculate Euclidean distance in original units\n",
    "euclidean_dist = np.linalg.norm(y_true_denorm - y_pred_denorm, axis=1).mean()\n",
    "print(f\"\\nTest Euclidean Distance (in meters): {euclidean_dist / 1000:.2f} m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
