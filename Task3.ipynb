{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9785e20a",
   "metadata": {},
   "source": [
    "Assignment 2 -Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213b6d2",
   "metadata": {},
   "source": [
    "Import Packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b60c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004076d",
   "metadata": {},
   "source": [
    " Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77ee8042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Edge Data:\n",
      "       source      target\n",
      "0  19585800.0  19590700.0\n",
      "1  19585800.0  19595200.0\n",
      "2  19590700.0  19592400.0\n",
      "3  19590700.0  19595200.0\n",
      "4  19591900.0  19592201.0\n",
      "\n",
      "Processed Node Data:\n",
      "    node_id  current_x  current_y  ... previous_y future_x  future_y\n",
      "0  19502500    40972.0   -16957.0  ...   -16957.0  41185.0  -16480.0\n",
      "1  19585800    12688.0    -6816.0  ...    -6816.0  13381.0   -7427.0\n",
      "2  19590700    12888.0    -6249.0  ...    -6249.0  13540.0   -6865.0\n",
      "3  19591900     8934.0    -3797.0  ...    -3797.0   9006.0   -4048.0\n",
      "4  19592201    10095.0    -5080.0  ...    -5080.0  10712.0   -5615.0\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Path to your dataset folder\n",
    "data_dir = r\"D:\\DL_Assignment2\\DL_Assignment2\\dataset (1)\\dataset\"\n",
    "\n",
    "\n",
    "def read_edge_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"source\", \"target\"]\n",
    "    return df\n",
    "\n",
    "# Function to read node information files\n",
    "def read_node_file(path):\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    df.columns = [\"node_id\", \"current_x\", \"current_y\", \"previous_x\", \"previous_y\", \"future_x\", \"future_y\"]\n",
    "    return df\n",
    "\n",
    "edge_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".edges\")]\n",
    "all_edges = pd.DataFrame()\n",
    "for path in edge_paths:\n",
    "    all_edges = pd.concat([all_edges, read_edge_file(path)], ignore_index=True)\n",
    "\n",
    "\n",
    "node_paths = [os.path.join(data_dir, fname) for fname in os.listdir(data_dir) if fname.endswith(\".nodes\")]\n",
    "all_nodes = pd.DataFrame()\n",
    "for path in node_paths:\n",
    "    all_nodes = pd.concat([all_nodes, read_node_file(path)], ignore_index=True)\n",
    "\n",
    "all_nodes.replace(to_replace='_', value=np.nan, inplace=True)\n",
    "\n",
    "required_fields = [\"future_x\", \"future_y\", \"previous_x\", \"previous_y\"]\n",
    "clean_nodes = all_nodes.dropna(subset=required_fields).reset_index(drop=True)\n",
    "\n",
    "all_edges.replace(-1, np.nan, inplace=True)\n",
    "all_edges.dropna(inplace=True)\n",
    "\n",
    "valid_node_ids = set(clean_nodes[\"node_id\"].unique())\n",
    "edge_nodes = set(all_edges[\"source\"]).union(set(all_edges[\"target\"]))\n",
    "missing_nodes = edge_nodes.difference(valid_node_ids)\n",
    "\n",
    "filtered_edges = all_edges[\n",
    "    ~all_edges[\"source\"].isin(missing_nodes) & ~all_edges[\"target\"].isin(missing_nodes)\n",
    "]\n",
    "\n",
    "reversed_edges = filtered_edges.rename(columns={\"source\": \"target\", \"target\": \"source\"})\n",
    "edges = pd.concat([filtered_edges, reversed_edges], ignore_index=True)\n",
    "\n",
    "# Final cleaned nodes and edges\n",
    "nodes = clean_nodes\n",
    "\n",
    "# Display the result\n",
    "print(\"Processed Edge Data:\")\n",
    "print(edges.head())\n",
    "\n",
    "print(\"\\nProcessed Node Data:\")\n",
    "print(nodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe39c6",
   "metadata": {},
   "source": [
    "Normalize IDs and Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57de5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Node Feature Values:\n",
      "   current_x  current_y  previous_x  previous_y\n",
      "0   0.925234   0.172395    0.927256    0.174749\n",
      "1   0.581439   0.405838    0.586045    0.413002\n",
      "2   0.583870   0.418890    0.588457    0.426323\n",
      "3   0.535809   0.475334    0.540757    0.483930\n",
      "4   0.549921   0.445800    0.554763    0.453787\n"
     ]
    }
   ],
   "source": [
    "node_id_map = {original_id: new_index for new_index, original_id in enumerate(sorted(nodes[\"node_id\"].unique()))}\n",
    "\n",
    "nodes[\"node_id\"] = nodes[\"node_id\"].map(node_id_map)\n",
    "edges[\"source\"] = edges[\"source\"].map(node_id_map)\n",
    "edges[\"target\"] = edges[\"target\"].map(node_id_map)\n",
    "\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "\n",
    "# Copy raw targets first\n",
    "raw_targets = nodes[[\"future_x\", \"future_y\"]].copy()  \n",
    "\n",
    "# Fit target scaler first using raw targets\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(raw_targets)\n",
    "\n",
    "# Now scale only future_x, future_y using the same scaler\n",
    "nodes[[\"future_x\", \"future_y\"]] = target_scaler.transform(raw_targets)\n",
    "\n",
    "# Apply feature scaling separately\n",
    "features_to_scale = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "feature_scaler = MinMaxScaler()\n",
    "nodes[features_to_scale] = feature_scaler.fit_transform(nodes[features_to_scale])\n",
    "\n",
    "\n",
    "print(\"Scaled Node Feature Values:\")\n",
    "print(nodes[features_to_scale].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddef780",
   "metadata": {},
   "source": [
    "Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c7824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(nodes.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = nodes.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = nodes.iloc[random_indices[len(random_indices) // 2 :]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e5b18",
   "metadata": {},
   "source": [
    "Prepare the Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5aaf26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape:\t\t (5840, 2)\n",
      "Node features shape: (2171, 4)\n"
     ]
    }
   ],
   "source": [
    "train_indices = train_data[\"node_id\"].to_numpy()\n",
    "test_indices = test_data[\"node_id\"].to_numpy()\n",
    "\n",
    "train_labels = train_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "test_labels = test_data[[\"future_x\", \"future_y\"]].astype(float).to_numpy()\n",
    "\n",
    "# Define graph representation\n",
    "edges = tf.convert_to_tensor(edges[[\"source\", \"target\"]].to_numpy(), dtype=tf.int32)\n",
    "node_states = tf.convert_to_tensor(nodes.sort_values(\"node_id\").iloc[:, 1:-2].to_numpy(), dtype=tf.float32)\n",
    "\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4ab38",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14d7f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        node_states_normalized = tf.math.l2_normalize(node_states_transformed, axis=-1)\n",
    "        similarity_scores = tf.matmul(node_states_normalized, tf.transpose(node_states_normalized))\n",
    "\n",
    "        mask = tf.eye(tf.shape(similarity_scores)[0])\n",
    "        similarity_scores = tf.where(mask == 1, tf.fill(tf.shape(similarity_scores), -np.inf), similarity_scores)\n",
    "\n",
    "        attention_scores = tf.nn.softmax(similarity_scores, axis=-1)\n",
    "        out = tf.matmul(attention_scores, node_states_transformed)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06639465",
   "metadata": {},
   "source": [
    "Multi-Head attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "074e0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [\n",
    "            attention_layer([atom_features, pair_indices])\n",
    "            for attention_layer in self.attention_layers\n",
    "        ]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60c9746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        \n",
    "        # Additional layers as per task 2\n",
    "        self.additional_layer1 = layers.Dense(hidden_units, activation=\"relu\")\n",
    "        self.additional_layer2 = layers.Dense(hidden_units, activation=\"relu\")\n",
    "        \n",
    "        self.attention_layers = [\n",
    "            MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(output_dim, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        x = self.additional_layer1(x)\n",
    "        x = self.additional_layer2(x)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) \n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.gather(outputs, indices)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed598582",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d043a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started: Task3\n",
      "Epoch 1/100\n",
      "4/4 - 9s - 2s/step - euclidean_dist: 0.8519 - loss: -2.6102e-02 - val_loss: -2.4965e-02\n",
      "Epoch 2/100\n",
      "4/4 - 2s - 548ms/step - euclidean_dist: 0.8470 - loss: -2.3906e-02 - val_loss: -2.1665e-02\n",
      "Epoch 3/100\n",
      "4/4 - 2s - 538ms/step - euclidean_dist: 0.8403 - loss: -2.0115e-02 - val_loss: -1.7128e-02\n",
      "Epoch 4/100\n",
      "4/4 - 2s - 547ms/step - euclidean_dist: 0.8326 - loss: -1.5299e-02 - val_loss: -1.1888e-02\n",
      "Epoch 5/100\n",
      "4/4 - 2s - 533ms/step - euclidean_dist: 0.8246 - loss: -9.9648e-03 - val_loss: -6.4096e-03\n",
      "Epoch 6/100\n",
      "4/4 - 2s - 525ms/step - euclidean_dist: 0.8160 - loss: -4.3371e-03 - val_loss: -5.4709e-04\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step\n",
      "\n",
      "Test Euclidean Distance (in meters): 65.88 m\n"
     ]
    }
   ],
   "source": [
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = 2\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "def euclidean_dist(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=-1))\n",
    "\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM, clipnorm=1.0)\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-5, patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "gat_model = GraphAttentionNetwork(\n",
    "    node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM\n",
    ")\n",
    "\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[euclidean_dist])\n",
    "\n",
    "print('Training started: Task3')\n",
    "\n",
    "x = gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "results = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0, return_dict=True)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_scaled = gat_model.predict(test_indices)\n",
    "y_true_scaled = test_labels\n",
    "\n",
    "# Inverse transform to get original values\n",
    "y_pred_denorm = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true_denorm = target_scaler.inverse_transform(y_true_scaled)\n",
    "\n",
    "# Calculate Euclidean distance in original units\n",
    "euclidean_dist = np.linalg.norm(y_true_denorm - y_pred_denorm, axis=1).mean()\n",
    "print(f\"\\nTest Euclidean Distance (in meters): {euclidean_dist / 1000:.2f} m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
